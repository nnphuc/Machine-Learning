{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cây quyết định\n",
    "**Cây quyết định (Decision Tree)** là một phương pháp học máy có giám sát không tham số được sử dụng để phân lớp và hồi quy.\n",
    "\n",
    "Mục đích của cây quyết định là tạo ra một mô hình dự đoán kết quả mục tiêu bằng cách học các luật quyết định đơn giản được suy diễn ra từ các đặc trưng dữ liệu.\n",
    "\n",
    "Mỗi tập luật định nghĩa ra một giả thuyết, có thể được biểu diễn bằng một cây quyết định với đường đi xuôi từ gốc đến lá cho ta một luật quyết định. Nút gốc và mỗi nút trên cây là một thuộc tính/ điều kiện kiểm tra, các nhánh đi xuống từ nút ứng với các giá trị có thể của thuộc tính/điều kiện này. Nhãn của các mẫu phù hợp là các nút lá.\n",
    "\n",
    "\n",
    "Hình dưới đây minh họa một cây quyết định của dữ liệu **Titanic** dự đoán khả năng sống sót khi tàu chìm.\n",
    "<img src=\"titanic.png\" style=\"text-align:center; max-height:400px\">\n",
    "\n",
    "**Bài tập:** Mô tả tập luật của cây quyết định trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trả lời**: *Điền đáp án vào đây!*\n",
    "```py\n",
    "if gender == male and age > 10:\n",
    "    return died\n",
    "if gender == male and age <= 10 and num_of_siblings > 2:\n",
    "    return died\n",
    "if gender == male and age <= 0 and num_of_siblings < =2:\n",
    "    return survived\n",
    "if gender == female:\n",
    "    return survived\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mô hình cây quyết định trong Scikit-learn\n",
    "Trong `Scikit-learn`, mô hình cây quyết định được cài đặt trong gói `tree` với `DecisionTreeClassifier`.\n",
    "\n",
    "**Bài tập:** Import dữ liệu và mô hình cây quyết định từ `Scikit-learn`, sau đó huấn luyện và biểu diễn mô hình thu được sau khi huấn luyện.\n",
    "\n",
    "*Gợi ý:* Sử dụng kiến thức từ bài thực hành trước với mô hình `SVM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'digraph Tree {\\nnode [shape=box, style=\"filled, rounded\", color=\"black\", fontname=helvetica] ;\\nedge [fontname=helvetica] ;\\n0 [label=<X<SUB>2</SUB> &le; 2.6<br/>gini = 0.665<br/>samples = 120<br/>value = [43, 40, 37]>, fillcolor=\"#e581390a\"] ;\\n1 [label=<gini = 0.0<br/>samples = 43<br/>value = [43, 0, 0]>, fillcolor=\"#e58139ff\"] ;\\n0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\\n2 [label=<X<SUB>2</SUB> &le; 4.75<br/>gini = 0.499<br/>samples = 77<br/>value = [0, 40, 37]>, fillcolor=\"#39e58113\"] ;\\n0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\\n3 [label=<gini = 0.0<br/>samples = 36<br/>value = [0, 36, 0]>, fillcolor=\"#39e581ff\"] ;\\n2 -> 3 ;\\n4 [label=<X<SUB>3</SUB> &le; 1.75<br/>gini = 0.176<br/>samples = 41<br/>value = [0, 4, 37]>, fillcolor=\"#8139e5e3\"] ;\\n2 -> 4 ;\\n5 [label=<X<SUB>2</SUB> &le; 4.95<br/>gini = 0.5<br/>samples = 8<br/>value = [0, 4, 4]>, fillcolor=\"#39e58100\"] ;\\n4 -> 5 ;\\n6 [label=<gini = 0.0<br/>samples = 3<br/>value = [0, 3, 0]>, fillcolor=\"#39e581ff\"] ;\\n5 -> 6 ;\\n7 [label=<X<SUB>3</SUB> &le; 1.65<br/>gini = 0.32<br/>samples = 5<br/>value = [0, 1, 4]>, fillcolor=\"#8139e5bf\"] ;\\n5 -> 7 ;\\n8 [label=<gini = 0.0<br/>samples = 4<br/>value = [0, 0, 4]>, fillcolor=\"#8139e5ff\"] ;\\n7 -> 8 ;\\n9 [label=<gini = 0.0<br/>samples = 1<br/>value = [0, 1, 0]>, fillcolor=\"#39e581ff\"] ;\\n7 -> 9 ;\\n10 [label=<gini = 0.0<br/>samples = 33<br/>value = [0, 0, 33]>, fillcolor=\"#8139e5ff\"] ;\\n4 -> 10 ;\\n}'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Import mô hình và dữ liệu cần thiết từ thư viện\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "# TODO: Chia dữ liệu huấn luyện và kiểm tra hợp lý\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target,test_size=0.2)\n",
    "\n",
    "# TODO: Huấn luyện mô hình với dữ liệu\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Visualize mô hình vừa được xây dựng với tập dữ liệu kiểm tra\n",
    "from IPython.display import Image ,display\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "dot_data = StringIO()\n",
    "\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "dot_data.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Các thuật toán xây dựng cây quyết định cơ bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vấn đề cơ bản của bài toán xây dựng cây quyết định là:\n",
    "- Xác định thuộc tính/điều kiện của mỗi nút\n",
    "- Thứ tự các nút\n",
    "\n",
    "Trong bài này, chúng ta sẽ làm quen với 2 thuật toán cơ bản nhất là **ID3** (Iterative Dichotomiser 3) và **C4.5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thuật toán ID3 (Quinlan 1986) chọn thuộc tính tốt nhất của tập huấn luyện được làm nút gốc theo tiêu chuẩn cực đại lượng thu hoạch thông tin (Information Gain). \n",
    "\n",
    "### Entropy\n",
    "Entropy dùng để đo độ không chắc chắn (độ mù mờ của thông tin). Nếu ta tập dữ liệu $D$ có $N$ phần tử, thuộc $C$ lớp và số phần tử mỗi lớp là $N_c$ thì entropy của tập dữ liệu $D$ được tính theo công thức:\n",
    "\n",
    "$$ E(D) = - \\sum_{c=1}^{C} \\frac{N_c}{N}\\log (\\frac{N_c}{N}) = - \\sum_{c=1}^{C}p_c\\log(p_c)$$\n",
    "**Bài tập**: Định nghĩa hàm `entropy(freq)` để tính entropy của phân phối xác suất dữ liệu `freq`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 2.5764258916820024\n"
     ]
    }
   ],
   "source": [
    "# TODO: Để có thể xây dựng được cây quyết định, việc đầu tiên cần làm là tính \n",
    "#       toán entropy cho dữ liệu với một phân phối cho trước (hoặc được tính\n",
    "#       toán thông qua dữ liệu)\n",
    "#       Định nghĩa hàm entropy(freq) tính toán độ mù mờ của dữ liệu với phân \n",
    "#       phối xác suất freq, là tần suất của mỗi lớp c trong bộ dữ liệu D. Hàm trả về số thực là độ đo entropy tương ứng.\n",
    "import numpy as np\n",
    "def entropy(freq):\n",
    "    log = np.log2(freq)\n",
    "    return -np.sum(log*freq)\n",
    "\n",
    "freq = np.array([0.2, 0.3, 0.12, 0.18, 0.08, 0.06, 0.06])\n",
    "print(\"Entropy = {}\".format(entropy(freq)))\n",
    "\n",
    "# Kết quả xấp xỉ 2.576"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy hai thuộc tính\n",
    "Khi thuộc tính $x_i$ được chọn làm nút, chia tập $D$ thành $K$ nhánh con $D_1, D_2,...,D_k$, số lượng phần tử trong mỗi nốt con kí hiệu là $m_k$. Độ đo entropy  sau phép chia này được tính:\n",
    "$$ E(D,x_i) = \\sum_{k=1}^{K} \\frac{m_k}{N}E(D_k)= \\sum_{k\\in K}P(k)E(D_k) $$\n",
    "\n",
    "**Bài tập:** Tính độ đo entropy khi có thêm một thuộc tính."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy with iris: 1.0289218221807177\n"
     ]
    }
   ],
   "source": [
    "# TODO: Khi chọn thêm một thuộc tính làm nốt chia, ta phải tính entropy với\n",
    "#       thuộc tính mới để tìm ra thuộc tính chia tốt nhất.\n",
    "#       Định nghĩa hàm _entropy(data, target, target_attr):\n",
    "#       - data (np.array): tập dữ liệu ban đầu\n",
    "#       - target(np.array): tập nhãn tương ứng với dữ liệu\n",
    "#       - target_attr(id): thuộc tính chia cần tính entropy\n",
    "# Gợi ý: Sử dụng lại hàm entropy()\n",
    "\n",
    "def distribution(target): #hàm tính phân phối của target\n",
    "    values,counts = np.unique(target,return_counts = True)\n",
    "    return values,counts/len(data)\n",
    "\n",
    "def _entropy(data, target, attr):\n",
    "    # tính entropy cho tập dữ liệu đầy đủ ban đầu\n",
    "    values, dist = distribution(target)\n",
    "    ent_full = entropy(dist)\n",
    "    \n",
    "    # chia nhỏ tập dữ liệu với thuộc tính chia\n",
    "    # mỗi giá trị của thuộc tính chia chia tập dữ liệu thành 1 tập con\n",
    "    small_sets = None\n",
    "    attr_values = data[...,attr]\n",
    "    \n",
    "    # tính entropy khi biết thêm thông tin về thuộc tính chia (công thức tính E(D,xi))\n",
    "    _ent = float(\"inf\")\n",
    "    for split_value in attr_values:\n",
    "        #chia target theo thuộc tính lấy thông tin\n",
    "        less = target[attr_values < split_value]\n",
    "        greater = target[attr_values >= split_value]\n",
    "        #entropy nhỏ hơn , lớn hơn hoặc bằng\n",
    "        ent = len(less)/len(data)*entropy(distribution(less)[1]) + len(greater)/len(data)*entropy(distribution(greater)[1])\n",
    "        _ent = min(ent,_ent)\n",
    "    \n",
    "    return _ent\n",
    "\n",
    "# Tính entropy cho dữ liệu hoa cẩm chướng khi chọn độ dài lá để chia\n",
    "iris_entropy = _entropy(data, target, 0)\n",
    "print(\"Entropy with iris: {}\".format(iris_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Độ thu hoạch thông tin\n",
    "Độ thu hoạch thông tin được tính là độ giảm entropy khi biết thêm một thông tin $x$:\n",
    "$$ Gain(D,x_i) = G(D,x_i)= E(D) - E(D,x_i) $$\n",
    "\n",
    "Thuộc tính nào cho độ mù mờ thông tin (entropy) nhỏ nhất hay có độ thu hoạch thông tin lớn nhất sẽ được chọn làm thuộc tính tại nút.\n",
    "\n",
    "$$ x^* = \\underset{x}{\\arg\\max}G(D,x_i) = \\underset{x}{\\arg\\min}E(D,x_i) $$\n",
    "**Bài tập:** Viết hàm tính độ thu hoạch thông tin khi thử chọn một thuộc tính làm thuộc tính chia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Dựa vào công thức ở trên, định nghĩa hàm gain(data, target, new_attr) tính độ\n",
    "#       thu hoạch thông tin khi chia nhỏ tập dữ liệu theo thuộc tính mới.\n",
    "def gain(data, target, attr):\n",
    "    # Tính entropy của tập dữ liệu\n",
    "    data_entropy = entropy(distribution(target)[1])\n",
    "    \n",
    "    # Tính entropy khi tập dữ liệu bị chia bởi thuộc tính mới\n",
    "    # Khi chọn thuộc tính lần thứ nhất, thuộc tính chia được chọn trước đó có id = -1\n",
    "    data_entropy_divide = _entropy(data,target,attr)\n",
    "    \n",
    "    # Tính độ thu hoạch thông tin\n",
    "    gain_infor = data_entropy - data_entropy_divide\n",
    "    \n",
    "    return gain_infor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài tập:** Với tập dữ liệu hoa cẩm chướng ban đầu, chọn ra thuộc tính chia tốt nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best attribute can be used: 2\n"
     ]
    }
   ],
   "source": [
    "# TODO: Dựa vào các hàm đã xây dựng trước đó, chọn ra nút gốc cho cây quyết định\n",
    "#       của hoa cẩm chướng (trả về chỉ số của thuộc tính trong tập dữ liệu)\n",
    "\n",
    "# chọn thuộc tính tốt nhất với thuật toán ID3\n",
    "\n",
    "best_attr = np.argmax([gain(data,target,attr) for attr in range(data.shape[1])])\n",
    "print(\"Best attribute can be used: {}\".format(best_attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5\n",
    "Thuật toán C4.5 được đề xuất năm 1993 bởi Quinlan nhằm khắc phục điểm yếu của thuật toán ID3: áp dụng Tỷ lệ thu hoạch thông tin cực đại (Gain Ratio).\n",
    "\n",
    "Tỷ lệ thu hoạch này phạt các thuộc tính có nhiều giá trị bằng cách thêm vào một hạng tử gọi là `thông tin chia` (Split Information), đại lượng này rất nhạy cảm với việc đánh giá tính rộng và đồng nhất khi chia tách dữ liệu theo giá trị thuộc tính:\n",
    "$$ SplitInformation(D,x_i)=-\\sum_{i=1}^{k} \\frac{\\left|D_i\\right|}{\\left|D\\right|} \\log{\\frac{\\left|D_i\\right|}{\\left|D\\right|}}$$\n",
    "`Split Information` thực tế là entropy của tập dữ liệu `D` ứng với thuộc tính chia `x_i`.\n",
    "\n",
    "Khi đó, tỷ lệ thông tin chia được tính bằng cách chia độ thu hoạch thông tin cho thông tin chia.\n",
    "$$ GainRatio(D, x_i) = \\frac{Gain(D,x_i)}{SplitInformation(D,x_i)} $$\n",
    "\n",
    "**Bài tập:** Hoàn thành hàm `split_infor(...)` tính thông tin chia và hàm `gain_ratio(...)` để tỷ lệ thu hoạch thông tin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Định nghĩa hai hàm split_infor(...) và gain_ratio(...) để cải thiện thuật toán \n",
    "#       ID3 theo ý tưởng của C4.5\n",
    "def split_infor(data, new_attr):\n",
    "    # chia dữ liệu ban đầu thành các tập nhỏ với new_attr\n",
    "    split_sets = distribution(data[...,new_attr]) \n",
    "    \n",
    "    # tính độ chia thông tin\n",
    "    split_inf = entropy(split_sets[1])\n",
    "    return split_inf\n",
    "\n",
    "def gain_ratio(data, target, new_attr):\n",
    "    # tính độ thu hoạch thông tin\n",
    "    gain_inf = gain(data,target,new_attr)\n",
    "    \n",
    "    # tính độ chia thông tin\n",
    "    split_inf = split_infor(data,new_attr)\n",
    "    \n",
    "    # áp dụng công thức, ta có tỷ lệ thông tin chia\n",
    "    gain_ratio = gain_inf / split_inf\n",
    "    \n",
    "    return gain_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Bài tập:** Dựa trên việc cải thiện thuật toán ID3, chọn lại nút gốc cho cây quyết định với dữ liệu hoa cẩm chướng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best attribute can be used: 3\n"
     ]
    }
   ],
   "source": [
    "# TODO: Dựa vào các hàm đã xây dựng trước đó, chọn ra nút gốc cho cây quyết định\n",
    "#       của hoa cẩm chướng (trả về chỉ số của thuộc tính trong tập dữ liệu)\n",
    "\n",
    "# chọn thuộc tính tốt nhất với thuật toán C4.5\n",
    "best_attr = np.argmax([gain_ratio(data,target,attr) for attr in range(data.shape[1])])\n",
    "print(\"Best attribute can be used: {}\".format(best_attr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
